%% **ğŸš€ åˆå§‹åŒ–ç¯å¢ƒ**
clc; clear; close all;

%% **ğŸ“¥ åŠ è½½ CSV æ•°æ®**
filename = 'Bitcoin_tweets.csv';
opts = detectImportOptions(filename, 'Delimiter', ',', 'VariableNamesLine', 1);
opts = setvaropts(opts, 'text', 'WhitespaceRule', 'preserve');
data = readtable(filename, opts);

% **å»é™¤é‡å¤å€¼**
data = unique(data, 'rows');
disp(['Shape after removing duplicates: ', num2str(size(data))]);

% **è½¬æ¢ 'date' åˆ—ä¸º datetime æ ¼å¼**
data.date = datetime(data.date, 'InputFormat', 'yyyy-MM-dd HH:mm:ss', 'Format', 'yyyy-MM-dd HH:mm:ss');

% **åˆ é™¤æ— æ•ˆæ—¥æœŸå’Œç©ºæ–‡æœ¬**
data = data(~isnat(data.date) & ~cellfun('isempty', data.text), :);
disp(['Shape after dropping invalid datetime values: ', num2str(size(data))]);

%% **ğŸ“– è¯»å– VADER è¯å…¸**
lexiconPath = 'vader_lexicon.txt';
opts = detectImportOptions(lexiconPath, 'Delimiter', '\t', 'ReadVariableNames', false);
opts.VariableNames = {'Word', 'Score', 'StdDev', 'Distribution'}; % ç¡®ä¿åˆ—åæ­£ç¡®
lexicon = readtable(lexiconPath, opts);

% **è½¬æ¢ 'Word' åˆ—ä¸ºå­—ç¬¦ä¸²æ•°ç»„**
lexicon.Word = string(lexicon.Word);

% **åˆ›å»ºæƒ…æ„Ÿè¯å…¸**
sentimentDict = containers.Map(lexicon.Word, lexicon.Score);

%% **ğŸ“ æ¸…ç†æ¨æ–‡**
clean_tweet = @(twt) regexprep(regexprep(regexprep(regexprep(regexprep( ...
    twt, '#bitcoin|#Bitcoin|#btc', 'bitcoin', 'ignorecase'), ...  % æ›¿æ¢#bitcoin
    '#[A-Za-z0-9]+', ''), ...                                    % ç§»é™¤å…¶ä»–ä¸»é¢˜æ ‡ç­¾
    '\n', ''), ...                                               % ç§»é™¤æ¢è¡Œç¬¦
    'https?://\S+', ''), ...                                     % ç§»é™¤é“¾æ¥
    '@\w+ *', '');                                              % ç§»é™¤æåŠç”¨æˆ·

% **åº”ç”¨æ¸…ç†**
data.CleanTwt = eraseURLs(data.text);            % åˆ é™¤ URL
data.CleanTwt = erasePunctuation(data.CleanTwt); % åˆ é™¤æ ‡ç‚¹ç¬¦å·
data.CleanTwt = lower(data.CleanTwt);            % è½¬æ¢ä¸ºå°å†™
data.CleanTwt = cellfun(clean_tweet, data.text, 'UniformOutput', false);

% **ç¡®ä¿åªä¿ç•™è‹±æ–‡å­—æ¯å’Œç©ºæ ¼**
data.CleanTwt = regexprep(data.CleanTwt, '[^a-zA-Z\s]', '');

% **åˆ é™¤æ¸…ç†åä¸ºç©ºçš„æ¨æ–‡**
data = data(~cellfun(@isempty, data.CleanTwt), :);

%% **ğŸ”„ åˆ†æ‰¹å¤„ç†æ¨æ–‡æƒ…æ„Ÿåˆ†æ**
n = height(data);
batch_size = 500000; % æ¯æ¬¡å¤„ç† 50 ä¸‡æ¡æ•°æ®
num_batches = ceil(n / batch_size); % è®¡ç®—æ€»æ‰¹æ¬¡

% **é¢„åˆ†é…å­˜å‚¨å˜é‡**
Sentiment = nan(n, 1);
SentimentMax = nan(n, 1);
SentimentMin = nan(n, 1);
SentimentMedian = nan(n, 1);
SentimentVar = nan(n, 1);
ExtremeSentimentCount = nan(n, 1);

% **å¯åŠ¨å¹¶è¡Œæ± **
num_workers = min(6, num_batches); % ç¡®ä¿å¹¶è¡Œæ± ä¸ä¼šè¶…è¿‡æ‰¹æ¬¡æ•°
% **æ£€æŸ¥å¹¶è¡Œæ± æ˜¯å¦å·²å­˜åœ¨**
if isempty(gcp('nocreate'))
    parpool(num_workers);
else
    disp('âœ… å¹¶è¡Œæ± å·²å­˜åœ¨ï¼Œç»§ç»­ä½¿ç”¨å½“å‰å¹¶è¡Œæ± ã€‚');
end


for batch_idx = 1:num_batches
    fprintf('ğŸš€ Processing batch %d/%d...\n', batch_idx, num_batches);
    
    % **è®¡ç®—å½“å‰æ‰¹æ¬¡èŒƒå›´**
    start_idx = (batch_idx - 1) * batch_size + 1;
    end_idx = min(batch_idx * batch_size, n);
    batch_indices = start_idx:end_idx;
    
    % **åˆ›å»ºå±€éƒ¨å˜é‡é¿å… parfor å¤åˆ¶æ•´ä¸ªè¡¨**
    tweetTexts = data.CleanTwt(batch_indices);
    batch_size_actual = numel(batch_indices);

    % **å¹¶è¡Œè®¡ç®—**
    batchSentiment = nan(batch_size_actual, 1);
    batchSentimentMax = nan(batch_size_actual, 1);
    batchSentimentMin = nan(batch_size_actual, 1);
    batchSentimentMedian = nan(batch_size_actual, 1);
    batchSentimentVar = nan(batch_size_actual, 1);
    batchExtremeSentimentCount = nan(batch_size_actual, 1);

    parfor i = 1:batch_size_actual
        documents = tokenizedDocument(tweetTexts{i}); % ä»¤ç‰ŒåŒ–æ¨æ–‡
        tokens = tokenDetails(documents);  % è·å–æ¨æ–‡çš„å•è¯åˆ—è¡¨
        wordsList = tokens.Token;  % æå–å•è¯
        scores = zeros(size(wordsList));

        % éå†æ¨æ–‡ä¸­çš„æ¯ä¸ªå•è¯ï¼ŒæŸ¥æ‰¾æƒ…æ„Ÿå¾—åˆ†
        for j = 1:numel(wordsList)
            word = string(wordsList{j});  % ç¡®ä¿å•è¯æ˜¯å­—ç¬¦ä¸²
            if isKey(sentimentDict, word)
                scores(j) = sentimentDict(word); % è·å–å•è¯çš„æƒ…æ„Ÿå¾—åˆ†
            end
        end

        % è®¡ç®—æ¨æ–‡çš„æƒ…æ„Ÿç»Ÿè®¡é‡
        if ~isempty(scores)
            batchSentiment(i) = mean(scores);  % è®¡ç®—å‡å€¼
            batchSentimentMax(i) = max(scores); % è®¡ç®—æœ€å¤§å€¼
            batchSentimentMin(i) = min(scores); % è®¡ç®—æœ€å°å€¼
            batchSentimentMedian(i) = median(scores); % è®¡ç®—ä¸­ä½æ•°
            batchSentimentVar(i) = var(scores); % è®¡ç®—æ–¹å·®
            batchExtremeSentimentCount(i) = sum(scores > 0.75 | scores < -0.75); % è®¡ç®—æç«¯æƒ…æ„Ÿ
        end
    end
    
    % **ä¿å­˜æ‰¹æ¬¡ç»“æœ**
    Sentiment(batch_indices) = batchSentiment;
    SentimentMax(batch_indices) = batchSentimentMax;
    SentimentMin(batch_indices) = batchSentimentMin;
    SentimentMedian(batch_indices) = batchSentimentMedian;
    SentimentVar(batch_indices) = batchSentimentVar;
    ExtremeSentimentCount(batch_indices) = batchExtremeSentimentCount;
end

%% **ğŸ“Š ç»Ÿè®¡æƒ…æ„Ÿå¾—åˆ†ï¼ˆæŒ‰åˆ†é’Ÿï¼‰**
data.Sentiment = Sentiment;
data.SentimentMax = SentimentMax;
data.SentimentMin = SentimentMin;
data.SentimentMedian = SentimentMedian;
data.SentimentVar = SentimentVar;
data.ExtremeSentimentCount = ExtremeSentimentCount;
data.Datetime = dateshift(data.date, 'start', 'minute');

% **è®¡ç®—å„ç§æƒ…æ„Ÿç»Ÿè®¡å€¼**
aggregated_stats = groupsummary(data, 'Datetime', {'mean', 'median', 'max', 'min', 'var'}, 'Sentiment');

%% **ğŸ’¾ ä¿å­˜æ•°æ®**
writetable(aggregated_stats, 'Sentiment_cleaned_1min.csv');
disp('âœ… Bitcoin Tweet Sentiment saved to CSV successfully.');

% **è·å–è¿‡å»90å¤©çš„æ•°æ®**
end_date = max(aggregated_stats.Datetime);
start_date = end_date - days(90);
filtered_sentiment = aggregated_stats(aggregated_stats.Datetime >= start_date & ...
    aggregated_stats.Datetime <= end_date, :);

% **ä¿å­˜è¿‡å»90å¤©çš„æ•°æ®**
writetable(filtered_sentiment, 'Sentiment_cleaned_90days_1min.csv');
disp('âœ… Filtered Bitcoin Tweet Sentiment for the last 90 days saved to CSV successfully.');

